<!DOCTYPE html>
<html lang="fa">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>تشخیص چهره‌ها و احساسات با face-api.js</title>
    <style>
      canvas {
        position: absolute;
        top: 0;
        left: 0;
      }
    </style>
  </head>
  <body>
    <h1>تشخیص چندین چهره و احساسات</h1>
    <video id="video" width="720" height="560" autoplay muted></video>
    <canvas id="overlay" width="720" height="560"></canvas>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.0"></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"
    ></script>

    <script>
      async function setupCamera() {
        const video = document.getElementById("video");
        const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
        video.srcObject = stream;
        return new Promise((resolve) => {
          video.onloadedmetadata = () => {
            resolve(video);
          };
        });
      }

      async function loadModels() {
        const MODEL_URL =
          "https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights";
        await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
        await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
      }

      async function detectFaces() {
        const video = document.getElementById("video");
        const canvas = document.getElementById("overlay");
        const displaySize = { width: video.width, height: video.height };
        faceapi.matchDimensions(canvas, displaySize);

        setInterval(async () => {
          const detections = await faceapi
            .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks()
            .withFaceExpressions();

          const resizedDetections = faceapi.resizeResults(detections, displaySize);
          canvas.getContext("2d").clearRect(0, 0, canvas.width, canvas.height);

          resizedDetections.forEach((detection) => {
            const { expressions } = detection;
            const box = detection.detection.box;

            const dominantExpression = Object.keys(expressions).reduce((a, b) =>
              expressions[a] > expressions[b] ? a : b
            );

            let drawBoxColor;
            switch (dominantExpression) {
              case "happy":
                drawBoxColor = "green";
                break;
              case "angry":
                drawBoxColor = "red";
                break;
              case "sad":
                drawBoxColor = "blue";
                break;
              case "surprised":
                drawBoxColor = "orange";
                break;
              case "fearful":
                drawBoxColor = "purple";
                break;
              case "disgusted":
                drawBoxColor = "brown";
                break;
              case "neutral":
              default:
                drawBoxColor = "pink";
                break;
            }

            const drawBox = new faceapi.draw.DrawBox(box, {
              label: dominantExpression,
              boxColor: drawBoxColor,
            });
            drawBox.draw(canvas);
          });
        }, 100);
      }

      async function run() {
        await loadModels();
        await setupCamera();
        detectFaces();
      }

      window.onload = () => {
        run();
      };
    </script>
  </body>
</html>
